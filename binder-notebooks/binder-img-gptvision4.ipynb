{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcace06f-ed25-41fe-986a-e78864dc9a71",
   "metadata": {},
   "source": [
    "This notebook reconstructs an image processing workflow recommended by GPT Vision 4 to better prepare a low-quality scan for OCR. The intention of this notebook is to critically evaluate AI-recommendations for image processing as opposed to earlier decisions taken by the DigiKAR team. The packages and operations below were explicitly recommended by GPT Vision 4, which reproduces the visual output that Monika Barget also received when prompting directly in the ChatGPT user interface. For the initial prompting, Monika Barget used a page from the 1740 \"Staatskalender\" published in the Electorate of Mainz. Instances where the script below diverts from the GPT Vision 4 recommendations are highlighted in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339bfbe5-fd40-4ff1-8377-98bd9b878788",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Import packages recommended by ChatGPT\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "# Confirm import for users\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937541-3a2e-45a1-b5f5-076cd540be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "GITHUB_REPO = \"https://github.com/YOUR_GITHUB_USERNAME/atr-historical-research.git\"  # Replace with your actual repo\n",
    "DATASET_PATH = \"/sample_data_jpg\"  # Replace with user_data_jpg when working with own samples\n",
    "OUTPUT_PATH = \"/tmp/processed_images\" # Temporary folder for processed images\n",
    "\n",
    "# Confirm when folder path exists\n",
    "print(\"Folder found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd83eba-3527-4dd9-a1b9-3f4ec4b4ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone dataset if not already present\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"Cloning dataset from GitHub...\")\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, DATASET_PATH])\n",
    "else:\n",
    "    print(\"Dataset already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da44c54-3f78-46c2-8773-c690c9a078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa8c4c-c6d2-452d-91a9-367f87d8df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first image\n",
    "first_image = os.path.join(DATASET_PATH, filenames[0])\n",
    "\n",
    "# Confirm selection for users\n",
    "print(f\"Using first found image: {first_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a86cec-9770-4073-97db-fcddae650183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image in grayscale (GPT-Vision-4 recommendation)\n",
    "image = cv2.imread(first_image, cv2.IMREAD_GRAYSCALE) # This AI-\"decision\" is problematic!\n",
    "\n",
    "# Error handling added for BYOD lab workshop\n",
    "if image is None:\n",
    "    raise ValueError(f\"Could not load image: {first_image}\")\n",
    "else:\n",
    "    print(\"Valid image loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4315a-aaaf-4b80-8e9a-83c5e0c2e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image enhancement workflow as created by GPT-Vision-4\n",
    "# users are welcome to adjust the values and test alternative results\n",
    "\n",
    "# The main library used here is OpenCV, which Monika Barget also used in her much shorter and simpler\n",
    "# script for the DigiKAR project. \n",
    "\n",
    "# OpenCV (Open Source Computer Vision Library) is an open source computer vision and\n",
    "# machine learning software library. We should discuss how many of its modules make sense for OCR image\n",
    "# preparation.\n",
    "\n",
    "# Full documentation: https://docs.opencv.org/4.x/index.html\n",
    "\n",
    "# Step 1: Noise Reduction using Non-Local Means Denoising\n",
    "denoised = cv2.fastNlMeansDenoising(image, h=30, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# Step 2: Adaptive Contrast Enhancement using CLAHE\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "contrast_enhanced = clahe.apply(denoised)\n",
    "\n",
    "# Step 3: Text-Specific Sharpening to Enhance Borders\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                   [-1, 5,-1],\n",
    "                   [0, -1, 0]])\n",
    "sharpened = cv2.filter2D(contrast_enhanced, -1, kernel)\n",
    "\n",
    "# Convert to PIL for saving and visualization\n",
    "enhanced_image = Image.fromarray(sharpened)\n",
    "\n",
    "# Save the processed image\n",
    "output_file = os.path.join(OUTPUT_PATH, \"enhanced_image.jpg\")\n",
    "enhanced_image.save(output_file)\n",
    "print(f\"Processed image saved to: {output_file}\")\n",
    "\n",
    "# Display the Original vs Enhanced Image\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sharpened, cmap='gray')\n",
    "plt.title(\"AI-Enhanced Image\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
